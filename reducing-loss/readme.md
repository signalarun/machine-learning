* [Stochastic Gradient Descent (SGD)](https://developers.google.com/machine-learning/glossary/#SGD)
* [RMSprop optimization algorithm](https://wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp)
* [Adam](https://wikipedia.org/wiki/Stochastic_gradient_descent#Adam) and [Adagrad](https://developers.google.com/machine-learning/glossary/#AdaGrad)

Using the RMSprop optimization algorithm, Adam and Adagrad is preferable to stochastic gradient descent (SGD), because RMSprop automates learning-rate tuning for us.
